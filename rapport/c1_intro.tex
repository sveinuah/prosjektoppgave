%===================================== CHAP 1 =================================

\chapter{Introduction} \label{chap:introduction}

As computational power of computers increase, and the processing and graphical units get smaller in size, there are more more focus on using cameras for control applications. Even though GPS based systems can calculate positions down to centimeter or even millimeter precision\cite{GPSaccuracy}, they will be less accurate in areas with large buildings, indoors, in tunnels and similar areas, because of line of sight requirements. Using sensor fusion with IMU sensors are a widespread approach to this, where you design filters or belief functions to estimate correct position and orientation. However IMUs are prone to drift in the measurements as a result of of noise and bias.

Cameras has the advantage that large parts of a scenery usually is stationary, providing vital information to counteract drift in orientation. However, as images only show a projection of the world, depth has to be estimated through motion or multi-camera setups. Combining camera and IMU for pose estimations are used in many applications for Visual Inertial Odomety and Visual SLAM. It is usually through either through Kalmann Filtering \cite{RealTimeKalmannSLAM, HighPrecKalmannVIO, OmniVIOKalman}, or through smoothing algorithms with factor graphs \cite{OnManiIntgrVIO, KeyFrameVIO}.

Many different approaches for pose estimation in VO and SLAM exist, for both direct methods and indirect methods. In direct methods \cite{DTAMdirect, LSDSLAMdirect}, photometric error between whole image frames are minimized to estimate the camera pose, while in indirect methods \cite{ORBSLAMindirect, 2yMarsndirect}, feature- extractors and descriptors are used to create features that are easy to find. These are then reprojected onto the images to create an error metric for the optimization. Some also combine the two approaches like the SVO-algorithm \cite{SVOpaper}, where they minimize the light intensity or photometric error on the pixels surrounding the chosen features.

In many cases, it is hard to distinguish features. Evenly colored walls, grasslands, or even direct sunlight can cause large problems for VO algorithms. Increasing the field of view is a way to increase the chances of finding good features, as well as ensuring that your landmarks stay in sight for a longer time. These qualities has lead to more and more experiments involving omnidirectional cameras in the later years \cite{CompOmniConvVSLAM, Zhang2016BenefitOL, OmniVIOKalman, OmniDenseSLAM}. Omnidirectional cameras are cameras with a $360^\circ$ degrees horizontal Field of View. While they capture more of the scene, the lenses or mirrors constructed to capture omnidirectional cause severe distortions to the image.

Rituerto et al. showed throught experiments that using a omnidirectional catadioptric camera, improved the performance of their SLAM algorithm significantly \cite{CompOmniConvVSLAM}. While a higher amount of pixels wad used with the omnidirectional image, as stated by Zhang et al. \cite{Zhang2016BenefitOL}. They did however find that omnidirectional cameras usually performed better with the SVO algorithm in indoor areas. Even with the same resolution across all FoVs tested. Which is backed up by Streckel et al. also found large field of view was beneficial for tracking indoors, using fisheye cameras \cite{OMNIChooseLensVisual}. Using their own Kalman filtering VIO approach and the synthetic urban canyon dataset by Zhang et. al. provided in \cite{Zhang2016BenefitOL}, Ramezani et. al. actually got better results with the $180^\circ$ FoV fisheye lens, than with the $90^\circ$ FoV perspective camera \cite{OmniVIOKalman}. They also had promising results with a real dataset from a $215^\circ$ fisheye lens mounted on top of a car pointing upwards.

Increasing the field of view requires an increase in image resolution, to uphold the same angular resolution, and therefore preserve the level of detail. However, increasing the resolution also comes at the cost of computational power. This is why we see relatively small camera resolutions used in SLAM and VO applications, while the commersial $360^\circ$ cameras sold today usually capture images at around 4k resolution. In \cite{MobileSLAM} they used a Samsung Galaxy S2 to do pose estimation with the captured images, while using a remote server to process the localization and mapping. Using a Mac book pro as their server and a resolution of 640x480 for the images, they managed around 2 seconds per keyframe for feature extraction, matching, triangulation and mapping. This shows how demanding SLAM applications can be, and the limitations that exist when using cameras for for example small UAVs. 

To develop and implement new applications for CV, a lot of simulations and tests has to be done. Testing on real equipment can however be expensive, especially in early development, showing the need for good simulators. Testing in simulators is not only cost effective, but often also time saving, as there is no need to get test equipment or travel to suited test sites. There are a lot of simulators supporting camera sensors. This paper will focus on the freely available ones like Gazebo \cite{GazeboPaper} and V-REP \cite{VREP2013}. As Gazebo is licenced under the Apache 2.0 licence, in addition to it's deep connection with ROS \cite{ROSpaper}, there have been also been developed simulators on top of Gazebo's source code. RotorS \cite{RotorS} is one example of this, specilized around MAVs. 

In later years, there has also been a lot of progress in making simulators based on powerful game engines. For example this simulator \cite{UnityROSsim} combining the game engine Unity with a ROS interface, to make a simulator for UAVs. Other examples include AirSim \cite{Airsim_paper} and Sim4CV \cite{Sim4CV_paper}, which are developed for Unreal Engine. All of these simulators implement cameras sensors as a core part of their platform. Modern game engines are made to provide realistic graphics rendering and shading, and it is especially the ability to create realistic shadows, sunshafts, reflections and direct light conditions, that make them appealing. All of these are real world effects which are hard to incorporate into Gazebo or V-REP. Game engines also provide powerful editing tools to edit scenery, 3D models and light conditions within the simulator, making them ideal for optimizing the simulations, without needing additional 3D modelling software. Adding these effects are however computationally demanding, which means that running these simulators require more powerful hardware, especially when it comes to graphical computations. 

Although there exists a lot of simulators, very few of them include good support for omnidirectional or wide angle camera captures. As most rendering engines are based around perspective projection, you cannot directly incorporate FoVs larger than $180^\circ$. There are limited ways to simulate the severe distortion in fisheye lenses and similar setups. Gazebo has a distortion model implementation, but it is still limited to $180^\circ$ FoV. Unity and Unreal Engine support cube captures, which captures 6 images in different directions, capturing the whole scene. There are however no support for custom distortion models. 

This paper will present a simulator for capturing images with fisheye lens cameras, made with the AirSim simulator for Unreal Engine 4. The camera will for now support customizable projection, using the polynomial model presented in chapter 11 of \cite{FisheyeCorke} \todo{Verify implementation}, which sould enable it to resemble most fisheye cameras. The fisheye camera will be supported by an interface to ROS, for creating publishing the images as ROS messages, as well as receive control inputs to the drone. This work is made to be iterated upon in my masters thesis.

\section{Report outline}

The report is mainly split into two parts. The first part contains theoretical background, as well as a small comparison of existing simulators to back the decision of going for Unreal Engine and AirSim. The other part covers the implementation details, challenges, experiments and further work. The outline is as follows:

\begin{itemize}
    \item \textbf{Chapter 2:} Theoretical background on camera projections and computer graphics, covering different lens models, rendering techniques and how they differ from each other.
    \item \textbf{Chapter 3:} Comparison of existing simulators for UAV simulations with camera sensors, focusing on the differences in graphics quality. This chapter also covers the implementation of the fisheye capture module, how it is attached to the AirSim simulator, general communication patterns for the simulator as a whole and challenges faced during the implementation.
    \item \textbf{Chapter 4:} Main results and discussion about the state of the simulator, its strengths and its weaknesses.
    \item \textbf{Chapter 5:} Conclusion to the report and a section on further work, leading to the master thesis.
    \item \textbf{Appendix:} TBC.
\end{itemize}

