%===================================== CHAP 1 =================================

\chapter{Introduction} \label{chap:introduction}

As computational power of computers increase, and the processing and graphical units get smaller in size, there are more focus on using cameras for control applications. Even though Global Navigation Satellite Systems(GNSS) can calculate positions down to centimeter or even millimeter precision\cite{GPSaccuracy}, they will be less accurate in areas with large buildings, indoors, in tunnels and similar areas, because of line of sight requirements to the satellite. 

Using sensor fusion with Inertial Measurement Unit(IMU) sensors is a widespread approach to this, where filters or belief functions are used to estimate correct position and orientation. However, IMU-based measurements are prone to drift as a result of of noise and bias.

Cameras have the advantage that large parts of a scenery usually is stationary, providing vital information to counteract drift in orientation. However, as images only show a projection of the world, depth has to be estimated through motion or multi-camera setups. Combining camera and IMU for pose estimations are used in many applications for Visual Inertial Odomety(VIO) and Visual Simultaneous Localization and Mapping(VSLAM). It is usually through either Extended Kalman Filtering \cite{RealTimeKalmannSLAM, HighPrecKalmannVIO, OmniVIOKalman}, or through smoothing algorithms with factor graphs \cite{OnManiIntgrVIO, KeyFrameVIO}.

Many different approaches for pose estimation in Visual Odometry(VO) and SLAM exist, for both direct methods and indirect methods. In direct methods \cite{DTAMdirect, LSDSLAMdirect}, photometric error between whole image frames are minimized to estimate the camera pose, while in indirect methods \cite{ORBSLAMindirect, 2yMarsndirect}, feature- extractors and descriptors are used to create features that are easy to find. These are then reprojected onto the images to create an error metric for the optimization. Some also combine the two approaches like the Semi-direct Visual Odometry(SVO)-algorithm \cite{SVOpaper}, where they minimize the light intensity or photometric error on the pixels surrounding the chosen features.

In many cases, it is hard to distinguish features. Evenly colored walls, grasslands, or even direct sunlight can cause large problems for VO algorithms. Increasing the field of view is a way to increase the chances of finding good features, as well as ensuring that your landmarks stay in sight for a longer time. These qualities has lead to more and more experiments involving omnidirectional cameras in the later years \cite{CompOmniConvVSLAM, Zhang2016BenefitOL, OmniVIOKalman, OmniDenseSLAM}. Omnidirectional cameras are cameras with a $360^\circ$ degrees horizontal FoV. While they capture more of the scene, they also induce severe distortion, caused by the lenses or constructed mirrors that are used.

The fact that omnidirectional catadioptric cameras can improve the performance of SLAM algorithms by a significant amount was shown by \cite{CompOmniConvVSLAM}. However, a higher resolution was used for the omnidirectional image, than for the perspective image in their simulations. In \cite{Zhang2016BenefitOL} they did found that omnidirectional cameras usually performed better with the SVO algorithm in indoor areas, even with the same resolution across all FoVs tested. This is backed up by \cite{OMNIChooseLensVisual}, who also found that a large field of view was beneficial for tracking indoors, using fisheye cameras. 

Through their own Kalman filtering VIO approach and the synthetic urban canyon dataset provided in \cite{Zhang2016BenefitOL}, the authors of \cite{OmniVIOKalman} also got better results with a $180^\circ$ FoV fisheye lens, than with the $90^\circ$ FoV perspective camera. They also showed promising results using a real dataset from a $185^\circ$ fisheye lens mounted on top of a car, with the camera pointing upwards, using a resolution of $1280 \times 720$.

Increasing the field of view requires an increase in image resolution, to uphold the same angular resolution, and therefore preserve the level of detail. However, increasing the resolution also comes at the cost of computational power. This is why relatively low camera resolutions are used in SLAM and VO applications, while the commercial $360^\circ$ cameras sold today usually capture images at around 4k resolution. In \cite{MobileSLAM} a Samsung Galaxy S2 was used to do pose estimation with the captured images, while a remote server was used to process the localization and mapping. Using a Mac book pro as their server and a resolution of 640x480 for the images, they managed around 2 seconds per keyframe for feature extraction, matching, triangulation and mapping. This shows how demanding SLAM applications can be, and the limitations that exist when using cameras for for example small Unmanned Aerial Vehicles(UAVs). 

In order to develop and implement new applications for Computer Vision(CV), a lot of simulations and tests has to be done. Testing on real equipment can be expensive, especially in early development, showing the need for good simulators. Using simulators are not only cost effective, but often also time saving, as there is no need to aquire test equipment or travel to suited test sites. Simulators can also show ground truth data, which provides more accurate test results. 

There are a lot of simulators supporting camera sensors, and many of them are freely available to use, such as Gazebo \cite{GazeboPaper} and V-REP \cite{VREP2013}. As Gazebo is licenced under the Apache 2.0 licence, in addition to it's deep connection with the Robot Operating System(ROS) \cite{ROSpaper}, multiple simulators have been developed on top of Gazebo's source code. RotorS \cite{RotorS} is one example of this, specialized around Micro Aerial Vehicles(MAVs). 

In later years, there has been a lot of progress in making simulators based on powerful game engines. For example the simulator in \cite{UnityROSsim} combining the game engine Unity with a ROS interface, to make a simulator for UAVs. Other examples include AirSim \cite{Airsim_paper} and Sim4CV \cite{Sim4CV_paper}, which are developed for Unreal Engine. All of these simulators implement camera sensors as a core part of their platform. Modern game engines are made to provide realistic graphics rendering and shading, and it is especially the ability to create realistic shadows, light shafts, reflections and direct light conditions that make them appealing. These are real world effects that are hard to incorporate into Gazebo or V-REP. Game engines also provide powerful editing tools to edit scenery, 3D models and light conditions within the simulator, making them ideal for optimizing the simulations without needing additional 3D modelling software. The addition these effects are computationally demanding, however, causing these simulators to require more powerful hardware, in order to properly execute the simulations.

Although there exists a lot of simulators, none of them include good support for omnidirectional or wide-angle camera captures. As most rendering engines are based on perspective projection, FoVs larger than $180^\circ$ cannot be captured directly. There are also limited ways to simulate the severe distortion seen in fisheye lenses and similar setups. Looking at game engines like Unreal Engine and Unity, they both support cube captures, consisting of six images taken in different directions. They do, however, not supply calibratable distortion models for these. Gazebo has a distortion model implementation, but it is limited to $180^\circ$ FoV of the perspective camera, making it unable to simulate many types of wide-angle cameras.

This paper will present a simulator for capturing images with fisheye lens cameras, made with the AirSim simulator for Unreal Engine 4. The camera will support a customizable projection, using the polynomial model presented in Chapter~11 of \cite{FisheyeCorke}, which will enable it to resemble most fisheye cameras. The fisheye camera will be supported by an interface to ROS, for publishing the images as messages to the ROS network.

\section{Report outline}

The report is mainly split into two parts. The first part contains theoretical background, as well as a small comparison of existing simulators to back the decision of going for Unreal Engine and AirSim. The second part covers the implementation details, challenges, experiments and further work. The outline is as follows:

\begin{itemize}
    \item \textbf{Chapter 2:} Theoretical background on camera projections and computer graphics, covering different lens models, rendering techniques and how they differ from each other.
    \item \textbf{Chapter 3:} Comparison of existing simulators for UAV simulations with camera sensors, focusing on the differences in graphics quality. This chapter also covers the implementation of the fisheye capture module, how it is attached to the AirSim simulator, general communication patterns for the simulator as a whole.
    \item \textbf{Chapter 4:} Main results and discussion about the state of the simulator, its strengths and its weaknesses.
    \item \textbf{Chapter 5:} Conclusion of the work presented in the report and further work, describing remaining work and presenting possibilities further extensions and uses.
    %\item \textbf{Appendix:} TBC.
\end{itemize}

