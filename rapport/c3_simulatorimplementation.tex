\section{System overview}

This project will be set up as three main modules: Unreal Engine, AirSim, and the fisheye camera. These are all compilable on their own and do not need to be compiled in any specific order. Only Unreal Engine is executable on its own. AirSim is indirectly run by Unreal Engine through adding the plugin to a specific unreal project.

The AirSim plugin is made in a way so that it technically resides outside of Unreal Engine. However, it has full access to the resources of the Engine, through the engine's header files. Their design choice involves adding as much functionality as possible outside of the Engine, to make it easier to support other game engines with only small changes to the AirSim package. The added features in the AirSim package includes a Multirotor and car models, perspective and depth camera, LIDAR, IMU with magnetometer and GPS. In addition to this, it has support for hardware in the loop(HITL) through the PX4/Pixhawk flight controller and MAVlink. 

As of now, AirSim is only usable with version 4.18 of Unreal Engine, while 4.21 is available at the time of writing. In order to reduce the chances of new commits to the original repositories causing problems for my project. Both AirSim and Unreal Engine 4.18 have been forked to a separate repository. This allows for full control over the updates made to the original repositories in addition to allowing custom modifications. The modifications to AirSim are available publicly. However, any changes to Unreal Engine must be kept private due to licensing.

\subsection{Communication pattern}

Because of the design choices of AirSim, much of the simulator computations happen outside of Unreal Engine. While Unreal Engine handles the core game loop, resource management, and the graphical tasks, custom AirSim scripts handle sensor simulation and kinematics. Of course, AirSim also relies on Unreal Engine for camera data, actor positions and pose, in addition to timing data. The AirSim plugin implements a game mode which is run by Unreal Engine on simulation start. This decides whether to do a multirotor simulation, car simulation or computer vision mode. The computer vision mode turns off physics, enabling free camera movement for creating datasets. The game mode also sets up the program loop run for each actor each frame.

The API AirSim consists of a server/client based system, using the remote procedure call(RPC\footnote{https://github.com/rpclib/rpclib}) library. This enables messages to be sent as network packages(TCP/IP), in addition to allowing interfaces made for multiple programming languages. The server is written in C++, and the client side is implemented for both Python and C++. As long as the server is enabled in the configuration file, it is started by Unreal Engine during the startup routine for the simulation. 

\begin{figure}[!htb]
    \centering
    \begin{tikzpicture}[node distance=2.2cm, transform shape, scale=0.7]
        \tikzstyle{topblob} = [ellipse, minimum width = 7cm, minimum height = 2cm, text centered, draw=black, fill=blue!10]
        \tikzstyle{blob} = [ellipse, minimum width = 3cm, minimum height = 1.5cm, text centered, draw=black, fill=red!20]
        \tikzstyle{emphblob} = [ellipse, minimum width = 3cm, minimum height = 1.5cm, text centered, draw=black, fill=red!50]
        \tikzstyle{ownblob} = [ellipse, minimum width = 3cm, minimum height = 1.5cm, text centered, draw=black, fill=green!10]
        \tikzstyle{doublearrow} = [thick, <->, >=stealth]
        \tikzstyle{tcp} = [dashed]
        
        \node (UE) [topblob]{Unreal Engine};
        \node (AirSim) [blob, below of=UE, align=center, yshift=-0.5cm]{AirSim \\ Plugin Core};
        \node (Sens) [blob, left of=AirSim, align=center, xshift=-3cm]{Sensor \\ Models};
        \node (Phys) [blob, below right of=AirSim, align=center, xshift=3cm]{Physics \\ Engine};
        \node (cam) [emphblob, above of=Phys, align=center, yshift=0.5cm]{\textbf{Camera} \\ \textbf{Interface}};
        \node (RPCS) [blob, below of=AirSim]{RPC Server};
        \node (RPCC) [blob, below of=RPCS]{RPC Client};
        \node (Client) [ownblob, below of=RPCC]{Core loop};
        \node (Fish) [ownblob, right of=RPCC, xshift=3cm, align=center]{Fisheye \\ Transform};
        \node (ROS) [ownblob, left of=RPCC, xshift=-3cm, align=center]{ROS Publisher};
        
        \draw[doublearrow] (UE) -- (AirSim);
        \draw[doublearrow] (UE) -- (Sens);
        \draw[doublearrow] (UE) -- (cam);
        \draw[doublearrow] (AirSim) -- (Phys);
        \draw[doublearrow] (AirSim) -- (cam);
        \draw[doublearrow] (AirSim) -- (Sens);
        \draw[doublearrow] (AirSim) -- (RPCS);
        \draw[tcp] (RPCS) -- (RPCC); \node (tcpText) [left of=RPCC, , xshift=1cm, yshift=1.1cm]{\scriptsize TCP/IP};
        \draw[doublearrow] (RPCC) -- (Client);
        \draw[doublearrow] (Client) -- (Fish);
        \draw[doublearrow] (Client) -- (ROS);
        
    \end{tikzpicture}
    \caption{Simplified code interaction within the simulator. Red nodes belong to the AirSim plugin, while Green nodes are custom made for this project. The emphasized node represent modified AirSim code.}
    \label{fig:comm_pattern}
\end{figure}

The project will be implemented for the multirotor game mode. This means that it will not run on the other game modes. The reason for this is that the cameras need to be set up specifically for each game mode, through the Unreal Engine Blueprints imported by the game mode. The fisheye camera and ROS interface will depend on the C++ RPC client API, the perspective camera model and the image capture module for AirSim. The contribution of the project will be to combine perspective images into a single fisheye lens-distorted image, with as few changes as possible to the AirSim plugin. In addition to this, it will build upon the C++ API to allow for publishing images to ROS, through the normal ROS publisher interface. Figure~\ref{fig:comm_pattern} shows the main communication flow of Unreal Engine, AirSim, and the fisheye camera. As the project does not involve HITL with PX4, Mavlink or any controller setup, they are not shown in the figure, and will not be discussed any further in this report.

\section{Implementation}

The goal of the implementation is to get a fisheye camera working and to publish the images to ROS. As only perspective cameras are available, and the fact that they are mounted to specific parts of the multirotor, it was not possible to implement this without making changes to AirSim. However, it was decided early on that no changes would be made to the source code of Unreal Engine. While some changes had to be made to the API in order to implement the ROS node, all of the old API is still functioning and usable.

While most of this report describes angles in degrees, all of the implementations are done using radians. This decision was made to ensure correct scaling when calculating distortion, as well as for consistency in order to reduce human errors.

\subsection{Early development} \label{sec:Early_dev}

As described in Section~\ref{sec:simulator_related}, Unreal Engine supports capturing of cube maps, which is a technique capturing the whole scene through rendering six $90^\circ$ FoV pictures. These are then stored in a special structure, with tooling for projecting it onto a sphere or to a plane using equireclinear projection. As the texture mapping tool for cube maps incorporate radial distortion, it was a natural starting point for simulating fisheye lenses. It is also used in \cite{UnrealCubeCapture}, where they describe using it in their omnidirectional video for their game.

There are various tutorials of how to capture $360^\circ$ screenshots, and many of them are using the Nvidia Ansel plugin \cite{SceenshotsAnsel}. While this made omnidirectional images of the scene, there was no way of processing the images directly, except by saving them locally and loading the images through OpenCV or by similar means. While this is definitely possible, it would create unnecessary save and load cycles. In addition to this, the performance of saving and loading images from disk varies heavily from computer to computer.

\begin{figure}[!htb]
    \centering
    \begin{tikzpicture}[node distance=3.5cm, transform shape, scale=0.7]
        \tikzstyle{topblob} = [ellipse, minimum width = 7cm, minimum height = 2cm, text centered, draw=black]
        \tikzstyle{blob} = [ellipse, minimum width = 3cm, minimum height = 1.5cm, text centered, draw=black]
        \tikzstyle{singlearrow} = [thick, ->, >=stealth]
        
        \node (cap) [topblob]{SceneCaptureComponent};
        \node (cap2d) [blob, below left of=cap, align=center]{SceneCapture- \\ Component2D};
        \node (cap3d) [blob, below right of=cap, align=center]{SceneCapture- \\ ComponentCube};
        \draw[singlearrow] (cap) -- (cap2d);
        \draw[singlearrow] (cap) -- (cap3d);
        
        \node (rend) [topblob, right of=cap, xshift=8cm]{TextureRenderTarget};
        \node (rend2d) [blob, below left of=rend, align=center]{TextureRender- \\ Target2D};
        \node (rend3d) [blob, below right of=rend, align=center]{TextureRender- \\ TargetCube};
        \draw[singlearrow] (rend) -- (rend2d);
        \draw[singlearrow] (rend) -- (rend3d);
        
    \end{tikzpicture}
    \caption{Unreal Engine Capture Component and Render Target class inheritance.}
    \label{fig:capture_render_inherit}
\end{figure}

The second approach involved modifying the capture request and processing system in AirSim directly, to allow requests of both perspective images and cube maps through the existing API. Each camera actor in Unreal Engine needs a capture component and a render target in order to know what to capture and where to store the data. In AirSim's camera setup and API, only the 2D version shown in Figure~\ref{fig:capture_render_inherit} is implemented. Adding support for cube captures would therefore either mean to write alternative cube capture versions of most of the modules shown in Figure~\ref{fig:comm_pattern_camera_request}, or handle both through types through pointers to their parent class.
 
Figure~\ref{fig:comm_pattern_camera_request} shows a simplified data flow in an image request for a perspective image. The arrows represent header file inclusions, where low opacity refers to function calls during setup and dashed arrows represent structure includes. The modules are simplified and many of them encapsulate multiple source code files.

\begin{figure}[!htb]
    \centering
    \tikzstyle{topblob} = [ellipse, minimum width = 7cm, minimum height = 2cm, text centered, draw=black, fill=blue!10]
    \tikzstyle{blob} = [ellipse, minimum width = 3cm, minimum height = 1.5cm, text centered, draw=black, fill=red!10]
    \tikzstyle{structs} = [rectangle, rounded corners, minimum width = 3cm, minimum height = 2cm, text centered, draw=black, fill=red!10]
    \tikzstyle{singlearrow} = [thick, ->, >=stealth]
    \tikzstyle{doublearrow} = [thick, <->, >=stealth]
    \tikzstyle{dashedarrow} = [thick, dashed, ->, >=stealth]
    \tikzstyle{tcp} = [dashed]
    
    \newcommand{\drawdashedarrow}{\raisebox{2pt}
    {\tikz{\draw[dashedarrow](0mm,0mm)--(4.25mm,0mm);}}}
    \newcommand{\drawarrow}{\raisebox{2pt}
    {\tikz{
        \draw[singlearrow](0mm,0mm)--(4.25mm,0mm);
        \draw[doublearrow](5mm,0mm)--(9.25mm,0mm);}}}
    \newcommand{\drawopacitydarrow}{\raisebox{2pt}
    {\tikz{
        \draw[singlearrow, opacity=0.5](0mm,0mm)--(4.25mm,0mm);
        \draw[doublearrow, opacity=0.5](5mm,0mm)--(9.25mm,0mm);}}}
    
    \begin{tikzpicture}[node distance=2.2cm, transform shape, scale=0.7]

        \node (UE) [topblob]{Unreal Engine};
        \node (PIP) [blob, below of=UE, align=center, yshift=-0.5cm]{Perspective \\ Camera};
        \node (setup) [blob, left of=PIP, align=center, xshift=-2cm]{Multirotor \\ Setup};
        \node (Render) [blob, right of=PIP, align=center, xshift=2cm]{Render \\ Request};
        \node (Conv) [blob, right of=Render, align=center, xshift=-0.5cm, yshift=2cm]{Image \\ Convertion};
        \node (UEIMC) [blob, below of=Render, align=center]{Unreal \\ ImageCapture};
        \node (API) [blob, below of=PIP]{Local API};
        \node (RPCS) [blob, below of=API]{RPC Server};
        \node (RPCC) [blob, below of=RPCS]{RPC Client};
        \node (Client) [blob, below of=RPCC]{Client API};
        \node (Struct) [structs, right of=RPCC, align=center, xshift=2cm, yshift=1.1cm]{ImageRequest/ \\ ImageResponse \\ structures};
        
        \draw[doublearrow] (UE) -- (PIP);
        \draw[doublearrow, opacity=0.5] (UE) -- (setup);
        \draw[singlearrow, opacity = 0.5] (PIP) -- (setup);
        \draw[singlearrow] (PIP) -- (UEIMC);
        \draw[singlearrow] (UE) -- (Render);
        \draw[singlearrow] (Conv) -- (Render);
        \draw[singlearrow] (Render) -- (UEIMC);
        \draw[singlearrow] (UEIMC) -- (API);
        \draw[singlearrow] (RPCS) -- (API);
        \draw[singlearrow, opacity=0.5] (setup) -- (API);
        \draw[tcp] (RPCS) -- (RPCC); \node (tcpText) [left of=RPCC, , xshift=1cm, yshift=1.1cm]{\scriptsize TCP/IP};
        \draw[singlearrow] (RPCC) -- (Client);
        \draw[dashedarrow] (Struct) -- (UEIMC);
        \draw[dashedarrow] (Struct) -- (API);
        \draw[dashedarrow] (Struct) -- (Client);
        
    \end{tikzpicture}
    \caption{Simplified overview of dependencies in an AirSim image request. Dashed arrows (\protect\drawdashedarrow) represent structure includes and whole arrows (\protect\drawarrow)represent function includes, low opacity arrows (\protect\drawopacitydarrow) represent dependencies during setup.}
    \label{fig:comm_pattern_camera_request}
\end{figure}

As writing alternative modules for AirSim to handle cube captures was deemed to be too large a task, the second option was chosen. Although this meant that more of the code was reusable, it quickly got out of hand. Since the respective child classes carry much of the implementation details for the capture modes, a lot of casting back and forth between types had to be implemented. This not only created code which was hard to read, but it also meant that much less of the original code was reusable than originally thought. Inexperience with advanced object-oriented C++ also created a huge time sink, using more time on fixing type-casting related problems than actual implementation.

\subsection{Final implementation overview} \label{subsec:Fisheye_impl_overview}

The final version of the fisheye camera is implemented as a client-side program, meaning that it is separated from both the AirSim plugin and Unreal Engine. This means that features of Unreal Engine that are not available through the AirSim API will no longer be available to the fisheye camera. As cube maps no longer are available, the fisheye image will have to be made by manually combining perspective images. Since there are no fisheye cameras with a vertical FoV greater than $270^\circ$, and the fact that the camera is mounted beneath the multirotor, the camera was implemented using a setup of 5 perspective cameras with a FoV of $90^\circ$. This can be extended to a full cube map, providing $360^\circ$ vertical FoV, by adding one extra camera to the multirotor model in Unreal Engine. The aspect ratio $\rho$ was also set to be $1$, matching that of normal cube maps. 
In order to be able to get the correct images, the camera setup on the multirotor model had to be changed. As seen in Figure~\ref{fig:new_Blueprint_multirotor}, the 5 cameras are clustered below the multirotor at a $90^\circ$ offset from one another, with the main direction being downwards. As the number of cameras and their name reference is hardcoded into AirSim, some changes had to be made to the camera interface. This consisted of adding references to the new cameras for the link between AirSim and Unreal Engine and new API entries for the AirSim camera API.

\begin{figure}[!htb]
    \centering
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[height=4cm]{rapport/fig/Simulator/camera_setup.png}
        \caption{Fisheye camera setup}
        \label{fig:new_Blueprint_cameras}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[height=4cm]{rapport/fig/Simulator/camera_setting.png}
        \caption{Downward camera settings}
        \label{fig:new_Blueprint_nodes}
    \end{subfigure}
    \caption{Multirotor Blueprint setup for the fisheye camera, consisting of 5 clustered perspective cameras.}
    \label{fig:new_Blueprint_multirotor}
\end{figure}

The following sections will present the modeling of the fisheye camera. The steps described are done on a pixel-wise level. As the same operation is done to every pixel in the source images, without any dependencies between them, this a highly parallelizable task, making it ideal for GPU acceleration.

\subsection{Combining pictures} \label{sec:combining_pictures}

The response form AirSim when requesting an image is an image response structure containing; camera name, position, orientation, time stamp, image dimensions, the image type, and the image data. It also contains whether the image has been compressed to a PNG format. This field is however irrelevant as the fisheye camera will request raw RGBA8 pictures in order to do the mapping.

\begin{figure}[!htb]
    \centering
    \tdplotsetmaincoords{70}{160}
    \begin{tikzpicture}[tdplot_main_coords, scale = 1.4]
    
    \coordinate (O) at (0,0,0);
    \tdplotsetrotatedcoords{0}{-90}{90}
    
    \coordinate (IMBOTC) at (0,0,-2);
    \tdplotsetrotatedcoordsorigin{(IMBOTC)}
    \draw[tdplot_rotated_coords, thick, fill=black!8, opacity=1.0] (2,0,2) -- (2,0,-2) -- (-2,0,-2) -- (-2,0,2) -- cycle;
    
    \coordinate (IMLEFT) at (0,-2,0);
    \tdplotsetrotatedcoordsorigin{(IMLEFT)}
    \draw[tdplot_rotated_coords, thick, fill=black!8, opacity=1.0] (0,2,2) -- (0,-2,2) -- (0,-2,-2) -- (0,2,-2) -- cycle;
    
    
    \tdplotsetrotatedcoordsorigin{(O)}
    
    \draw[tdplot_rotated_coords, ->] (0,0,2) -- (0,0,3) node[below]{$Z$};
    
    \tdplotsetcoord{IMGC}{2}{-90}{0}
    \tdplotsetrotatedcoordsorigin{(IMGC)}
    \draw[thick, tdplot_rotated_coords, fill=black!4, opacity=0.8] (-2,-2,0) -- (-2,2,0) -- (2,2,0) -- (2,-2,0) -- cycle;
    
    \draw[tdplot_rotated_coords, dashed] (0,0,-1.5) -- (0,0,0);
    
    \draw[tdplot_rotated_coords, dashed, opacity=0.8] (0,0,-2) -- (0,-2,0);
    \draw[tdplot_rotated_coords, dashed, opacity=0.8] (0,0,-2) -- (2,0,0);
    \draw[tdplot_rotated_coords, dashed, opacity=0.8] (0,0,-2) -- (2,2,0);
    \draw[tdplot_rotated_coords, dashed, opacity=0.8] (0,0,-2) -- (2,-2,0);
    \draw[tdplot_rotated_coords, dashed, opacity=0.8] (0,0,-2) -- (-2,-2,0);
    
    \tdplotsetrotatedcoordsorigin{(O)}
    \tdplotsetrotatedcoords{90}{70}{0}
    \draw[tdplot_rotated_coords, thick, fill=black!5, opacity = 0.8] (0:2) arc (0:180:2);
    \tdplotsetrotatedcoords{0}{-90}{90}
    \draw[thick, tdplot_rotated_coords, fill=black!3] (0:2) arc (0:360:2);
    
    \tdplotsetrotatedcoords{0}{-90}{90}
    \draw[tdplot_rotated_coords, ->] (-0.5,0,0) -- (3,0,0) node[below]{$X$};
    \draw[tdplot_rotated_coords, ->] (0,-0.5,0) -- (0,2.5,0) node[right]{$Y$};
    \draw[tdplot_rotated_coords, dashed] (0,0,0) -- (0,0,0.6);
    
    \draw[tdplot_rotated_coords, dashed, opacity=0.8] (1.155,1.155,1.155) -- (2,2,2);
    \draw[tdplot_rotated_coords, dashed, opacity=0.8] (1.155,0,1.155) -- (2,0,2);
    \draw[tdplot_rotated_coords, dashed, opacity=0.8] (1.155,-1.155,1.155) -- (2,-2,2);
    
    \draw[tdplot_rotated_coords, dashed, opacity=0.8] (0,0,0) -- (0.5,0.5,0.5);
    \draw[tdplot_rotated_coords, dashed, opacity=0.8] (0,0,0) -- (0.5,0,0.5);
    \draw[tdplot_rotated_coords, dashed, opacity=0.8] (0,0,0) -- (0.5,-0.5,0.5);
    \draw[tdplot_rotated_coords, dashed, opacity=0.8] (0,0,0) -- (0,-0.6,0.6);
    \draw[tdplot_rotated_coords, dashed, opacity=0.8] (0,0,0) -- (-0.7,-0.7,0.7);
    
    
    \tdplotsetcoord{IMTOPC}{2}{0}{-90}
    \tdplotsetrotatedcoordsorigin{(IMTOPC)}
    \draw[tdplot_rotated_coords, thick, fill=black!2, opacity=0.7] (2,0,2) -- (2,0,-2) -- (-2,0,-2) -- (-2,0,2) -- cycle;
    
    \coordinate (IMRIGHT) at (0,2,0);
    \tdplotsetrotatedcoordsorigin{(IMRIGHT)}
    \draw[tdplot_rotated_coords, thick, fill=black!2, opacity=0.7] (0,2,2) -- (0,-2,2) -- (0,-2,-2) -- (0,2,-2) -- cycle;
    
    \end{tikzpicture}
    
    \caption{Projection from perspective camera to a unit sphere using 5 perspective cameras, given a FoV of $180^\circ$.}
    \label{fig:impl_Sphere_projection}
\end{figure}

Figure~\ref{fig:impl_Sphere_projection} shows the setup for the custom cube capture used by the fisheye camera, where each plane represents an image taken by a camera on the multirotor. To provide a correct projection, the angles $\phi$ and $\theta$, shown in Figure~\ref{fig:fisheye_spherical_projection} must be known. In order to convert the pixel positions to these angles, it is beneficial to change to image coordinates on a plane with $x^i,y^i \in [-1,1]$. Here $[x^i, y^i]^\top$ and $[u^i,v^i]^\top$ refer to the image and pixel coordinates on the image plane represented, with the origin in each camera's own coordinate frame. Table~\ref{tab:impl_quaternion_rotations} shows the respective superscripts used to describe each camera frame. Using Equation~\eqref{eq:pixel_transform}, with each camera having an aspect ratio $\rho = 1$, the transformation shown in Equation~\eqref{eq:impl_pixel_inverse_transform} is obtained.

\begin{equation}
    \mathbf{p}^i = \begin{bmatrix}
        x^i \\ y^i \\ 1
    \end{bmatrix} = \begin{bmatrix}
        \frac{W}{2} & 0 & \frac{W}{2} \\
        0 & \frac{H}{2} & \frac{H}{2} \\
        0 & 0 & 1
    \end{bmatrix}^{-1}\begin{bmatrix}
        u^i \\ v^i \\ 1
    \end{bmatrix} = \begin{bmatrix}
        \frac{2}{W} & 0 & -1 \\
        0 & \frac{2}{H} & -1 \\
        0 & 0 & 1
    \end{bmatrix}\begin{bmatrix}
        u^i \\ v^i \\ 1
    \end{bmatrix}
    \label{eq:impl_pixel_inverse_transform}
\end{equation}

A coordinate frame $\bullet^c$ is defined for the fisheye $360^\circ$ camera to be aligned with the coordinate frame of the downward pointing camera. Hence all points $\mathbf{p}^i = [x^i,y^i,z^i]^\top$, where $i$ refers to the superscript assigned to each camera frame, should be rotated to the camera frame. This rotation is shown in Equation~\eqref{eq:impl_rotation}. $\mathbf{R}^d_i$ represents the rotation from fra $i$ to $d$, where $\mathbf{q}_{d,i}$ in Equation~\eqref{eq:impl_rotation_q} represent the same rotation on quaternion form.

\begin{subequations}
    \begin{align}
        \mathbf{p}^c &= \!\begin{aligned}[t]
            \mathbf{p}^d = \begin{bmatrix} x^d \\ y^d \\ z^d \end{bmatrix} &= \mathbf{R}^d_i \mathbf{p}^i
        \end{aligned} \label{eq:impl_rotation_R} \\[0.75ex]
        \mathbf{q}^c &= \!\begin{aligned}[t]
            \mathbf{q}^d = \begin{bmatrix} 0 \\ \mathbf{p}^d \end{bmatrix} &= (\mathbf{q}_{d,i})\mathbf{q}^i(\mathbf{q}_{d,i})^\top
        \end{aligned} \label{eq:impl_rotation_q}
    \end{align}
    \label{eq:impl_rotation}
\end{subequations}

The quaternion rotation in Equation~\eqref{eq:impl_rotation_q} will be implemented in this project. Table~\ref{tab:impl_quaternion_rotations} lists each camera, their defined coordinate frame, as well as the rotation needed for the representative image coordinate vector. $\hat{\theta}$ and $\hat{\phi}$ represent the rotation angle around the local $x-$ and $y-$axis respectively.

\begin{table}[!htb]
    \centering
    \caption{Rotation needed to rotate image coordinate $p^i$ to $p^d$.}
    \label{tab:impl_quaternion_rotations}
    \begin{tabular}{|c|c|c|c|} \hline
        \multirow{2}{*}{Camera Direction} & \multirow{2}{*}{ $\mathbf{p}^i$} & \multirow{2}{*}{$\mathbf{R}^d_i = \mathbf{R}_{x,\hat{\theta}}(\hat{\theta})\mathbf{R}_{y,\hat{\phi}}(\hat{\phi})$} & \multirow{2}{*}{Quaternion $\mathbf{q}_{d,i}$} \\ &&& \\ \hline \hline
        \multirow{2}{*}{Down} & \multirow{2}{*}{$\mathbf{p}^d$} & \multirow{2}{*}{$\hat{\theta} = 0$, $\hat{\phi} = 0$} & \multirow{2}{*}{$[1,0,0,0]^\top$} \\ &&& \\ \hline
        \multirow{2}{*}{Front} & \multirow{2}{*}{$\mathbf{p}^f$} & \multirow{2}{*}{$\hat{\theta} = \frac{\pi}{2}$, $\hat{\phi} = 0$} & \multirow{2}{*}{$\left[\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2},0,0\right]^\top$} \\ &&& \\ \hline
        \multirow{2}{*}{Right} & \multirow{2}{*}{$\mathbf{p}^r$} & \multirow{2}{*}{$\hat{\theta} = \frac{\pi}{2}$, $\hat{\phi} = \frac{\pi}{2}$} & \multirow{2}{*}{$[0.5,0.5,0.5,0.5]^\top$} \\ &&& \\ \hline
        \multirow{2}{*}{Back} & \multirow{2}{*}{$\mathbf{p}^b$} & \multirow{2}{*}{$\hat{\theta} = \frac{\pi}{2}$, $\hat{\phi} = \pi$} & \multirow{2}{*}{$\left[0,0,\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2}\right]^\top$} \\ &&& \\ \hline
        \multirow{2}{*}{Left} & \multirow{2}{*}{$\mathbf{p}^l$} & \multirow{2}{*}{$\hat{\theta} = \frac{\pi}{2}$, $\hat{\phi} = -\frac{\pi}{2}$} & \multirow{2}{*}{$\left[0.5,0.5,-0.5,-0.5\right]^\top$} \\ &&& \\ \hline
    \end{tabular}
\end{table}

After rotating the image coordinate to the camera frame, the feature unit vector $[\theta, \phi]^\top$ can be calculated using Equation~\eqref{eq:theory_polar_coords}. This calculation is shown in Equation~\eqref{eq:impl_polar_transform}. The arc tangent is implemented using the standard library \emph{atan2(y,x)}-function. This provides extra functionality for calculating the quadrant of the angle. The result is therefore in the range $[-\pi,\pi]$, which is needed for correct picture placement.

\begin{subequations}
    \begin{equation}
        \theta = atan2\left(y^c,x^c\right)
        \label{eq:impl_polar_transform_theta}
    \end{equation}
    \begin{equation}
        \phi = atan2\left(\sqrt{(x^c)^2 + (y^c)^2},z^c\right)
        \label{eq:impl_polar_transform_phi}
    \end{equation}
    \label{eq:impl_polar_transform}
\end{subequations}

\subsection{Lens implementation} \label{sec:lens_modeling}

The camera lens is implemented with the polynomial model presented in Table~11.1 in \cite{FisheyeCorke}. For now this is implemented as the fourth order polynomial in Equation~\eqref{eq:impl_lens_model}, where the parameters $k_1$-$k_4$ are tunable distortion parameters. $r(\phi)$ and $\theta$ refer to the polar coordinates of the fisheye image plane, defining the feature position in the fisheye image.

\begin{align}
    \begin{aligned}[t]
    r(\phi) &= k_1 \phi + k_2 \phi^2 + k_3 \phi^3 + k_4 \phi^4 \\[0.75ex]
    \theta &= \theta
    \end{aligned}
    \label{eq:impl_lens_model}
\end{align}

The polynomial model is flexible and easy to implement, while able to simulate most effects caused by radial distortion. Unfortunately, as the model does not have any dependency on the longitude $\theta$, it is not able to simulate the effects of tangential distortion and this will have to be implemented in the future.

In order to produce the final picture, the polar coordinates have to be transformed back to pixel coordinates for the new image. Again using $W$ as the width and $H$ as the height in pixels, the transformation can be found using Equation~\eqref{eq:pixel_transform}, as well as the relationship between polar and Cartesian coordinates shown in Equation~\eqref{eq:theory_equiangular_coords}. The resulting transformation is shown in Equation~\eqref{eq:impl_pixel_transform_final}.

\begin{equation}
    \begin{bmatrix}
        u \\ v \\ 1
    \end{bmatrix} = \begin{bmatrix}
        \frac{W}{2x_{max}} & 0 & \frac{W}{2} \\
        0 & \frac{H}{2y_{max}} & \frac{H}{2} \\
        0 & 0 & 1
    \end{bmatrix}\begin{bmatrix}
        x \\ y \\ 1
    \end{bmatrix} =
    \begin{bmatrix}
        \frac{W}{2x_{max}} & 0 & \frac{W}{2} \\
        0 & \frac{H}{2y_{max}} & \frac{H}{2} \\
        0 & 0 & 1
    \end{bmatrix}\begin{bmatrix}
        r(\phi) cos(\theta) \\ r(\phi) sin(\theta) \\ 1
    \end{bmatrix}
    \label{eq:impl_pixel_transform_final}
\end{equation}

Assuming the the fisheye image will the whole square image, $y_{max} = x_{max}$ can be calculated to be calculated as shown in Equation~\eqref{eq:impl_size_xy_max}, with $\phi_{max}=\frac{\Theta}{2}=\frac{3 \pi}{2}$.

\begin{equation}
    \begin{aligned}
        x_{max} = y_{max} &= ||k_1 \phi_{max} + k_2 \phi_{max}^2 + k_3 \phi_{max}^3 + k_4 \phi_{max}^4 ||_2 \\
        &= \left|\left| \left(\frac{3 \pi}{4}\right)k_1 + \left(\frac{3 \pi}{4}\right)^2 k_2 + \left(\frac{3 \pi}{4}\right)^3 k_3 + \left(\frac{3 \pi}{4}\right)^4 k_4 \right|\right|_2
    \end{aligned}
    \label{eq:impl_size_xy_max}
\end{equation}

If this is not the case, meaning that there is a mismatch between the size of the image plane and the size of the photosensitive element of the camera, then $x_{max}$ and $y_{max}$ needs to be calculated based on the pixel dimensions of the image sensor. This calculation is shown in Equation~\eqref{eq:impl_size_xy_max_nomatch}, where $w$ is the pixel width and $h$ is the pixel height.

\begin{equation}
    \begin{aligned}
        x_{max} &= w\frac{W}{2} \\
        y_{max} &= h \frac{H}{2}
    \end{aligned}
    \label{eq:impl_size_xy_max_nomatch}
\end{equation}


% \subsection{Code structure}

% The fisheye module is made to be included as a library into other projects, along with it's header file. The module defines a FisheyeTransformer class and the Lens, SourceImage and TeansformRequest structures. SourceImage contains the one perspective 

% \begin{figure}
%     \centering
%     \begin{tikzpicture}[node distance = 4cm, transform shape, scale=1.0]
%         \tikzstyle{class} = [rectangle, minimum width=3cm, minimum height = 3cm, text centered, draw=black, fill=blue!10]
%         \tikzstyle{struct} = [rectangle, minimum width=3cm, minimum height = 3cm, text centered, draw=black, fill=green!10]
        
%         \node (FET) [class, align=center, rectangle split, rectangle split parts=2]{
%             \textbf{FisheyeTransformer}
%             \nodepart{second} \begin{flushleft}+\end{flushleft} test
%         };
        
%     \end{tikzpicture}
%     \caption{Caption}
%     \label{fig:my_label}
% \end{figure} Add this?

\subsection{ROS interface to AirSim and fisheye camera}\label{subsec:ROS_interface}

The ROS node implementation is split into two different parts. The first part handles the core loop of the program, sending AirSim ImageRequests and handling the fisheye transformations, while the other handles ROS publishing. To be able to do this the ROS package inherits the base RPC-Client API class defined for AirSim, along with including the ROS header, to create a link between the two. However, for the added functionality of publishing fisheye images, it also depends on the fisheye module, OpenCV and cv\_bridge\footnote{https://github.com/ros-perception/vision\_opencv}, where cv\_bridge handles the conversion between the OpenCV image object and the ROS image message.

\begin{figure}[!htb]
    \centering
    \begin{tikzpicture}[node distance=2cm, transform shape, scale=0.7]
        \tikzstyle{start} = [rectangle, rounded corners, minimum width = 2cm, minimum height = 1cm, text centered, draw=black, fill=red!10]
        \tikzstyle{if} = [diamond, minimum width = 2.5cm, minimum height = 1.5cm, text centered, draw=black, fill=blue!10]
        \tikzstyle{task} = [rectangle, rounded corners, minimum width = 3cm, minimum height = 1.5cm, text centered, draw=black, fill=blue!10]
        \tikzstyle{singlearrow} = [thick, ->, >=stealth]
        
        \node (START) [start]{Start};
        \node (CONN) [task, below of=START, align=center, yshift=0.45cm]{Connect to \\ mutlirotor};
        \node (PUBINIT) [task, below of=CONN, align=center]{Initialize \\ Publisher};
        
        \node (ROSOK) [if, below of=PUBINIT, yshift=-0.8cm]{ros::ok()?};
        \draw[] (ROSOK) node[xshift=1.7cm, yshift=0.3cm]{yes};
        \draw[] (ROSOK) node[xshift=0.5cm, yshift=-1.5cm]{no};
        \node (REQ) [task, align=center, right of=ROSOK, xshift=1.7cm]{Request \\ Images};
        \node (OPENCV) [task, align=center, right of=REQ, xshift=1.3cm]{Convert to \\ cv::Mat};
        \node (FISH) [task, align=center, right of=OPENCV, xshift=1.3cm]{Create \\ fisheye image};
        \node (CONV) [task, align=center, right of=FISH, xshift=1.75cm]{Convert to ROS \\ sensor\_msgs/Image};
        \node (PUB) [task, align=center, right of=CONV, xshift=1.75cm]{Publish to \\ /FisheyeImage};
        
        \node (DISC) [task, align=center, below of=ROSOK, yshift=-0.5cm]{Disconnect from \\ multirotor};
        \node (END) [start, below of=DISC, yshift=0.45cm]{End};
        
        \draw[singlearrow] (START) -- (CONN);
        \draw[singlearrow] (CONN) -- (PUBINIT);
        \draw[singlearrow] (PUBINIT) -- (ROSOK);
        \draw[singlearrow] (ROSOK) -- (REQ);
        \draw[singlearrow] (REQ) -- (OPENCV);
        \draw[singlearrow] (OPENCV) -- (FISH);
        \draw[singlearrow] (FISH) -- (CONV);
        \draw[singlearrow] (CONV) -- (PUB);
        \path[draw, -latex, singlearrow] (PUB) -- ++(0cm,1.7cm) -| (ROSOK);
        \draw[singlearrow] (ROSOK) -- (DISC);
        \draw[singlearrow] (DISC) -- (END);
        
    \end{tikzpicture}
    \caption{ROS publisher loop.}
    \label{fig:impl_ROS_pub_loop}
\end{figure}

Using the custom setup discussed in Section~\ref{subsec:Fisheye_impl_overview}, the requests for the perspective images are treated by referencing the cameras by name, as there is no way of polling AirSim to get the available cameras. The received vector of images is then converted to OpenCV images and wrapped into a custom FisheyeImageRequest made for this project. This structure holds the images, image size, and information about the camera pose, which is necessary to compute the final image.

The created ROS node runs an infinite loop, shown in Figure~\ref{fig:impl_ROS_pub_loop} and it is interruptable by crtl+c through the ros::ok() function. The loop consists of polling for images, transformation to fisheye-image, and publishing the image on the FisheyImage ROS topic. In order to control the multirotor, the old client API needs to be included and used in the core loop of the client application. Both the ROS publisher and the old client API can run simultaneously.


\cleardoublepage