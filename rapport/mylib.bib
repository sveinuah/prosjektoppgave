@article{Airsim_paper,
  author    = {Shital Shah and
               Debadeepta Dey and
               Chris Lovett and
               Ashish Kapoor},
  title     = {AirSim: High-Fidelity Visual and Physical Simulation for Autonomous
               Vehicles},
  journal   = {CoRR},
  volume    = {abs/1705.05065},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.05065},
  archivePrefix = {arXiv},
  eprint    = {1705.05065},
  timestamp = {Mon, 13 Aug 2018 16:46:28 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ShahDLK17},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  note      = {Github repository: github.com/Microsoft/AirSim},
}

@article{Sim4CV_paper,
  author    = {Matthias Mueller and
               Vincent Casser and
               Jean Lahoud and
               Neil Smith and
               Bernard Ghanem},
  title     = {UE4Sim: {A} Photo-Realistic Simulator for Computer Vision Applications},
  journal   = {CoRR},
  volume    = {abs/1708.05869},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.05869},
  archivePrefix = {arXiv},
  eprint    = {1708.05869},
  timestamp = {Mon, 13 Aug 2018 16:48:37 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-05869},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Zhang2016BenefitOL,
  title={Benefit of large field-of-view cameras for visual odometry},
  author={Zichao Zhang and Henri Rebecq and Christian Forster and Davide Scaramuzza},
  journal={2016 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2016},
  pages={801-808}
}

@inproceedings{endoscopypano,
author = {Patrice Roulet and Pierre Konen and Mathieu Villegas and Simon Thibault and Pierre Y. Garneau},
title = {360$^{\circ}$ endoscopy using panomorph lens technology},
booktitle = {Conference Proceedings of SPIE},
journal = {Proc.SPIE},
volume = {7558},
number = {},
year = {2010},
doi = {10.1117/12.842417},
URL = {https://doi.org/10.1117/12.842417},
eprint = {}
}

@online{GPSaccuracy,
author      = {{National Coordination Office for Space-Based Positioning, Navigation, and Timing}},
title       = {GPS Accuracy},
date        = {2016-01-14},
url         = {https://www.gps.gov/systems/gps/performance/accuracy/}
}

@inproceedings{SVOpaper, 
author={C. Forster and M. Pizzoli and D. Scaramuzza}, 
booktitle={2014 IEEE International Conference on Robotics and Automation (ICRA)}, 
title={SVO: Fast semi-direct monocular visual odometry}, 
date={2014-05}, 
volume={}, 
number={}, 
pages={15-22}, 
keywords={autonomous aerial vehicles;control engineering computing;distance measurement;embedded systems;motion estimation;probability;robot vision;stereo image processing;fast semidirect monocular visual odometry;open-source software;consumer laptop;onboard embedded computer;GPS-denied environments;micro-aerial-vehicle state-estimation;high frame-rate motion estimation;3D points;outlier measurements;probabilistic mapping method;subpixel precision;pixel intensities;SVO;Cameras;Three-dimensional displays;Feature extraction;Robustness;Motion estimation;Tracking;Optimization}, 
doi={10.1109/ICRA.2014.6906584}, 
ISSN={1050-4729}
}

@online{raytraceblog,
author      = {Brian Caulfield},
title       = {What's the Difference Between Ray Tracing and Rasterization?},
date        = {2018-03-19},
url         = {https://blogs.nvidia.com/blog/2018/03/19/whats-difference-between-ray-tracing-rasterization/}
}

@INPROCEEDINGS{carsmovie, 
author      = {P. H. Christensen and J. Fong and D. M. Laur and D. Batali}, 
booktitle   = {2006 IEEE Symposium on Interactive Ray Tracing}, 
title       = {Ray Tracing for the Movie `Cars'}, 
year        = {2006}, 
volume      = {}, 
number      = {}, 
pages       = {1-6}, 
keywords    = {cache storage;cinematography;computational geometry;humanities;image texture;ray tracing;rendering (computer graphics);ray tracing;extended Pixar RenderMan renderer;multiresolution geometry;texture caches;movie-quality rendering;scanline rendering algorithms;Pixar Cars movie;Ray tracing;Motion pictures;Layout;Rendering (computer graphics);Optical reflection;Geometry;Production;Testing;Animation;Tiles}, 
doi         = {10.1109/RT.2006.280208}, 
ISSN        = {}, 
month       = {9},
}

@inproceedings{wald2009state,
title           = {State of the art in ray tracing animated scenes},
author          = {Wald, Ingo and Mark, William R and G{\"u}nther, Johannes and Boulos, Solomon and Ize, Thiago and Hunt, Warren and Parker, Steven G and Shirley, Peter},
booktitle       = {Computer graphics forum},
volume          = {28},
number          = {6},
pages           = {1691--1722},
year            = {2009},
organization    = {Wiley Online Library}
}

@article{wachter2006instant,
title       = {Instant ray tracing: The bounding interval hierarchy},
author      = {W{\"a}chter, Carsten and Keller, Alexander},
journal     = {Rendering Techniques},
volume      = {2006},
pages       = {139--149},
year        = {2006}
}

@article{UnrealCV,
  title = {UnrealCV: Virtual Worlds for Computer Vision},
  author = {Qiu, Weichao and Zhong, Fangwei and Zhang, Yi and Qiao, Siyuan and Xiao, Zihao and Kim, Tae Soo and Wang, Yizhou and Yuille, Alan},
  journal = {ACM Multimedia Open Source Software Competition},
  year = {2017}
}

@online{AirsimGit,
  title = {Microsoft Airsim Github repository},
  year = 2017,
  url = {https://github.com/Microsoft/AirSim}
}

@online{NvidiaConference,
title           = {NVIDIA GeForce RTX - Official Launch Event},
date            = {2018-08-20},
organization    = {NVIDIA Corporation},
author          = {Jensen Huang},
url             = {https://www.youtube.com/watch?v=Mrixi27G9yM},
note            = {Youtube video published by the NVIDIA GeForce channel},
}

@online{Gazebo_phys,
title           = {New Feature Highlight: Multiple Physics Engines},
date            = {2014-04-11},
organization    = {Open Source Robotics Foundation},
author          = {},
url             = {http://gazebosim.org/blog/feature_physics}}

@inproceedings{ROSpaper,
  title={ROS: an open-source Robot Operating System},
  author={Quigley, Morgan and Conley, Ken and Gerkey, Brian and Faust, Josh and Foote, Tully and Leibs, Jeremy and Wheeler, Rob and Ng, Andrew Y},
  booktitle={ICRA workshop on open source software},
  volume={3},
  number={3.2},
  pages={5},
  year={2009},
  organization={Kobe, Japan}
}

@online{Gazeboweb,
title = {Gazebo Simulator},
year = {2014},
url         = {http://gazebosim.org}
}

@online{Unityweb,
    title = {Unity 3D Game Engine},
    year = 2005,
    url = {http://unity3d.com}}
}

@online{Unrealweb,
    title = {Unreal Engine 4 Game Engine},
    year = 2004,
    url = {https://www.unrealengine.com}
}

@inproceedings{GazeboPaper,
  title={Design and use paradigms for Gazebo, an open-source multi-robot simulator.},
  author={Koenig, Nathan P and Howard, Andrew},
  organization={Citeseer}
}

@online{OpenCVweb,
    title = {Open Source Computer Vision library, OpenCV},
    year = 2000,
    url = {https://opencv.org/}
}

@Inbook{RotorS,
    author = {Furrer, Fadri and Burri, Michael and Achtelik, Markus and Siegwart, Roland},
    editor = {Koubaa, Anis},
    title = {RotorS---A Modular Gazebo MAV Simulator Framework},
    bookTitle = {Robot Operating System (ROS): The Complete Reference (Volume 1)},
    year = {2016},
    publisher = {Springer International Publishing},
    address = {Cham},
    pages = {595--625},
    abstract = {In this chapter we present a modular Micro Aerial Vehicle (MAV) simulation framework, which enables a quick start to perform research on MAVs. After reading this chapter, the reader will have a ready to use MAV simulator, including control and state estimation. The simulator was designed in a modular way, such that different controllers and state estimators can be used interchangeably, while incorporating new MAVs is reduced to a few steps. The provided controllers can be adapted to a custom vehicle by only changing a parameter file. Different controllers and state estimators can be compared with the provided evaluation framework. The simulation framework is a good starting point to tackle higher level tasks, such as collision avoidance, path planning, and vision based problems, like Simultaneous Localization and Mapping (SLAM), on MAVs. All components were designed to be analogous to its real world counterparts. This allows the usage of the same controllers and state estimators, including their parameters, in the simulation as on the real MAV.},
    isbn = {978-3-319-26054-9},
    doi = {10.1007/978-3-319-26054-9_23},
    url = {https://doi.org/10.1007/978-3-319-26054-9_23}
}

@InProceedings{VREP2010,
    author = {Freese, Marc and Singh, Surya and Ozaki, Fumio and Matsuhira, Nobuto},
    editor = {Ando, Noriaki and Balakirsky, Stephen and Hemker, Thomas and Reggiani, Monica and von Stryk, Oskar},
    title = {Virtual Robot Experimentation Platform V-REP: A Versatile 3D Robot Simulator},
    booktitle = {Simulation, Modeling, and Programming for Autonomous Robots},
    year = {2010},
    publisher = {Springer Berlin Heidelberg},
    address = {Berlin, Heidelberg},
    pages = {51--62},
    abstract = {From exploring planets to cleaning homes, the reach and versatility of robotics is vast. The integration of actuation, sensing and control makes robotics systems powerful, but complicates their simulation. This paper introduces a modular and decentralized architecture for robotics simulation. In contrast to centralized approaches, this balances functionality, provides more diversity, and simplifies connectivity between (independent) calculation modules. As the Virtual Robot Experimentation Platform (V-REP) demonstrates, this gives a small-footprint 3D robot simulator that concurrently simulates control, actuation, sensing and monitoring. Its distributed and modular approach are ideal for complex scenarios in which a diversity of sensors and actuators operate asynchronously with various rates and characteristics. This allows for versatile prototyping applications including systems verification, safety/remote monitoring, rapid algorithm development, and factory automation simulation.},
    isbn = {978-3-642-17319-6}
}

@inproceedings{VREP2013,
  title={V-REP: A versatile and scalable robot simulation framework},
  author={Rohmer, Eric and Singh, Surya PN and Freese, Marc},
  booktitle={Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International Conference on},
  pages={1321--1326},
  year={2013},
  organization={IEEE}
}

@inproceedings{HNMSimPaper,
  title={HNMSim: A 3D multi-purpose hybrid networked multi-agent simulator},
  author={Xu, Jun and Xie, Lihua and Toh, Teow Ghee and Toh, Yue Khing},
  booktitle={Control Conference (CCC), 2012 31st Chinese},
  pages={5915--5920},
  year={2012},
  organization={IEEE}
}

@inproceedings{UnityROSsim,
  title={ROS+ unity: An efficient high-fidelity 3D multi-UAV navigation and control simulator in GPS-denied environments},
  author={Meng, Wei and Hu, Yuchao and Lin, Jiaxin and Lin, Feng and Teo, Rodney},
  booktitle={Industrial Electronics Society, IECON 2015-41st Annual Conference of the IEEE},
  pages={002562--002567},
  year={2015},
  organization={IEEE}
}

@online{GazeboWideWeb,
    title = {Gazebo : Tutorial : Wide-Angle Camera},
    url = {http://gazebosim.org/tutorials?tut=wide_angle_camera&branch=wideanglecamera},
    urldate = {2018-10-26},
    year = {2015}
}

@article{CompOmniVSLAM,
  title={Comparison of omnidirectional and conventional monocular systems for visual slam},
  author={Rituerto, Alejandro and Puig, Luis and Guerrero, JJ},
  journal={10th OMNIVIS with RSS},
  year={2010}
}

@inproceedings{OmniVIOKalman,
  title={Omnidirectional visual-inertial odometry using multi-state constraint Kalman filter},
  author={Ramezani, Milad and Khoshelham, Kourosh and Kneip, Laurent},
  booktitle={Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on},
  pages={1317--1323},
  year={2017},
  organization={IEEE}
}

@online{UnityCubeCapture,
    title = {Stereo 360 Image and Video Capture in Unity},
    author = {Chow, Mike},
    url = {https://blogs.unity3d.com/2018/01/26/stereo-360-image-and-video-capture},
    date = {2018-01-26},
    urldate = {2018-10-26}
}

@online{UnrealCubeCapture,
    title = {Capturing Stereoscopic 360 Screenshots and Movies from Unreal Engine 4},
    author = {Costello, Gavin},
    url = {https://www.unrealengine.com/en-US/tech-blog/capturing-stereoscopic-360-screenshots-videos-movies-unreal-engine-4},
    date = {2016-05-10},
    urldate = {2018-10-26},
}

@online{NASAGazeboppt,
    title = {Gazebo Renders the moon presentation},
    author = {Chen, Ian and Allan, Mike},
    url = {https://roscon.ros.org/2018/presentations/ROSCon2018_gazeborendersmoon.pdf},
    date = {2018-09-29},
    urldate = {2019-01-13},
}

@online{Unrealshowcase,
    title = {Making fo Unreal Engine Forest Scene - Tip of the Week},
    author = {Franczak, Michal},
    url = {https://evermotion.org/tutorials/show/10765/making-of-unreal-engine-forest-scene-tip-of-the-week},
    date = {2017-07-27},
    urldate = {2019-01-13}
}



@online{Unityshowcase,
    title = {The Old Watch (photo-realistic Unity 5 Environment},
    author = {Goodwin, Pat},
    url = {https://www.artstation.com/artwork/a1G5z},
    year = {2017},
    urldate = {2019-01-13}
}

@inproceedings{panini,
  title={Pannini: a new projection for rendering wide angle perspective images},
  author={Sharpless, Thomas K and Postle, Bruno and German, Daniel M},
  booktitle={Proceedings of the Sixth international conference on Computational Aesthetics in Graphics, Visualization and Imaging},
  pages={9--16},
  year={2010},
  organization={Eurographics Association}
}

@article{KeyFrameVIO,
author  = {Stefan Leutenegger and Simon Lynen and Michael Bosse and Roland Siegwart and Paul Furgale},
title   ={Keyframe-based visual–inertial odometry using nonlinear optimization},
journal = {The International Journal of Robotics Research},
volume  = {34},
number  = {3},
pages   = {314-334},
year    = {2015},
doi     = {10.1177/0278364914554813},
URL     = {https://doi.org/10.1177/0278364914554813},
eprint  = {https://doi.org/10.1177/0278364914554813},
        
abstract = { Combining visual and inertial measurements has become popular in mobile robotics, since the two sensing modalities offer complementary characteristics that make them the ideal choice for accurate visual–inertial odometry or simultaneous localization and mapping (SLAM). While historically the problem has been addressed with filtering, advancements in visual estimation suggest that nonlinear optimization offers superior accuracy, while still tractable in complexity thanks to the sparsity of the underlying problem. Taking inspiration from these findings, we formulate a rigorously probabilistic cost function that combines reprojection errors of landmarks and inertial terms. The problem is kept tractable and thus ensuring real-time operation by limiting the optimization to a bounded window of keyframes through marginalization. Keyframes may be spaced in time by arbitrary intervals, while still related by linearized inertial terms. We present evaluation results on complementary datasets recorded with our custom-built stereo visual–inertial hardware that accurately synchronizes accelerometer and gyroscope measurements with imagery. A comparison of both a stereo and monocular version of our algorithm with and without online extrinsics estimation is shown with respect to ground truth. Furthermore, we compare the performance to an implementation of a state-of-the-art stochastic cloning sliding-window filter. This competitive reference implementation performs tightly coupled filtering-based visual–inertial odometry. While our approach declaredly demands more computation, we show its superior performance in terms of accuracy. }
}

@ARTICLE{OnManiIntgrVIO,
author  = {C. Forster and L. Carlone and F. Dellaert and D. Scaramuzza},
journal = {IEEE Transactions on Robotics},
title   = {On-Manifold Preintegration for Real-Time Visual--Inertial Odometry},
year    = {2017},
volume  = {33},
number  = {1},
pages   = {1-21},
keywords= {computer vision;computerised instrumentation;distance measurement;graph theory;inertial systems;maximum likelihood estimation;nonlinear programming;on-manifold preintegration theory;real-time visual-inertial odometry;nonlinear optimization;maximum a posteriori state estimation;preintegrated inertial measurement unit model;factor graph;incremental-smoothing algorithm;visual measurement;structureless model;monocular VIO pipeline evaluation;Smoothing methods;Optimization;Estimation;Real-time systems;Manifolds;Computational modeling;Jacobian matrices;Computer vision;sensor fusion;visual--inertial odometry (VIO)},
doi     = {10.1109/TRO.2016.2597321},
ISSN    = {1552-3098},
month   = {02},
}

@article{HighPrecKalmannVIO,
author  = {Mingyang Li and Anastasios I. Mourikis},
title   ={High-precision, consistent EKF-based visual-inertial odometry},
journal = {The International Journal of Robotics Research},
volume  = {32},
number  = {6},
pages   = {690-711},
year    = {2013},
doi     = {10.1177/0278364913481251},

URL     = {https://doi.org/10.1177/0278364913481251},
eprint  = {https://doi.org/10.1177/0278364913481251},

abstract = { In this paper, we focus on the problem of motion tracking in unknown environments using visual and inertial sensors. We term this estimation task visual–inertial odometry (VIO), in analogy to the well-known visual-odometry problem. We present a detailed study of extended Kalman filter (EKF)-based VIO algorithms, by comparing both their theoretical properties and empirical performance. We show that an EKF formulation where the state vector comprises a sliding window of poses (the multi-state-constraint Kalman filter (MSCKF)) attains better accuracy, consistency, and computational efficiency than the simultaneous localization and mapping (SLAM) formulation of the EKF, in which the state vector contains the current pose and the features seen by the camera. Moreover, we prove that both types of EKF approaches are inconsistent, due to the way in which Jacobians are computed. Specifically, we show that the observability properties of the EKF’s linearized system models do not match those of the underlying system, which causes the filters to underestimate the uncertainty in the state estimates. Based on our analysis, we propose a novel, real-time EKF-based VIO algorithm, which achieves consistent estimation by (i) ensuring the correct observability properties of its linearized system model, and (ii) performing online estimation of the camera-to-inertial measurement unit (IMU) calibration parameters. This algorithm, which we term MSCKF 2.0, is shown to achieve accuracy and consistency higher than even an iterative, sliding-window fixed-lag smoother, in both Monte Carlo simulations and real-world testing. }
}

@article{RealTimeKalmannSLAM,
title   = "Real-time implementation of airborne inertial-SLAM",
journal = "Robotics and Autonomous Systems",
volume  = "55",
number  = "1",
pages   = "62 - 71",
year    = "2007",
note    = "Simultaneous Localisation and Map Building",
issn    = "0921-8890",
doi     = "https://doi.org/10.1016/j.robot.2006.06.006",
url     = "http://www.sciencedirect.com/science/article/pii/S0921889006001485",
author  = "Jonghyuk Kim and Salah Sukkarieh",
keywords = "Airborne SLAM, Inertial Measurement Unit (IMU), Vision, UAV",
abstract = "This paper addresses some challenges to the real-time implementation of Simultaneous Localisation and Mapping (SLAM) on a UAV platform. When compared to the implementation of SLAM in 2D environments, airborne implementation imposes several difficulties in terms of computational complexity and loop closure, with high nonlinearity in both vehicle dynamics and observations. An implementation of airborne SLAM is formulated to relieve this computational complexity in both direct and indirect ways. Our implementation is based on an Extended Kalman Filter (EKF), which fuses data from an Inertial Measurement Unit (IMU) with data from a passive vision system. Real-time results from flight trials are provided."
}

@INPROCEEDINGS{DTAMdirect,
author  = {R. A. Newcombe and S. J. Lovegrove and A. J. Davison},
booktitle = {2011 International Conference on Computer Vision},
title   = {DTAM: Dense tracking and mapping in real-time},
year    = {2011},
volume  = {},
number  = {},
pages   = {2320-2327},
keywords={augmented reality;cameras;concave programming;graphics processing units;image motion analysis;image reconstruction;image texture;object tracking;dense tracking and mapping;real-time camera tracking;real-time camera reconstruction;hand-held RGB camera;textured depth map;video stream;photometric data term;energy functional;nonconvex optimisation;image alignment;GPU hardware;dense model;real-time scene interaction;physics-enhanced augmented reality;Cameras;Tracking;Real time systems;Image reconstruction;Vectors;Robustness;Optimization},
doi     = {10.1109/ICCV.2011.6126513},
ISSN    ={2380-7504},
month   = {11},
}

@InProceedings{LSDSLAMdirect,
author="Engel, Jakob
and Sch{\"o}ps, Thomas
and Cremers, Daniel",
editor="Fleet, David
and Pajdla, Tomas
and Schiele, Bernt
and Tuytelaars, Tinne",
title="LSD-SLAM: Large-Scale Direct Monocular SLAM",
booktitle="Computer Vision -- ECCV 2014",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="834--849",
abstract="We propose a direct (feature-less) monocular SLAM algorithm which, in contrast to current state-of-the-art regarding direct methods, allows to build large-scale, consistent maps of the environment. Along with highly accurate pose estimation based on direct image alignment, the 3D environment is reconstructed in real-time as pose-graph of keyframes with associated semi-dense depth maps. These are obtained by filtering over a large number of pixelwise small-baseline stereo comparisons. The explicitly scale-drift aware formulation allows the approach to operate on challenging sequences including large variations in scene scale. Major enablers are two key novelties: (1) a novel direct tracking method which operates on {\$}{\backslash}mathfrak{\{}sim{\}}(3){\$}, thereby explicitly detecting scale-drift, and (2) an elegant probabilistic solution to include the effect of noisy depth values into tracking. The resulting direct monocular SLAM system runs in real-time on a CPU.",
isbn="978-3-319-10605-2"
}

@article{ORBSLAMindirect,
  title={ORB-SLAM: a versatile and accurate monocular SLAM system},
  author={Mur-Artal, Raul and Montiel, Jose Maria Martinez and Tardos, Juan D},
  journal={IEEE Transactions on Robotics},
  volume={31},
  number={5},
  pages={1147--1163},
  year={2015},
  publisher={IEEE}
}

@article{2yMarsndirect,
author = {Maimone, Mark and Cheng, Yang and Matthies, Larry},
title = {Two years of Visual Odometry on the Mars Exploration Rovers},
journal = {Journal of Field Robotics},
volume = {24},
number = {3},
pages = {169-186},
doi = {10.1002/rob.20184},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.20184},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.20184},
abstract = {Abstract NASA's two Mars Exploration Rovers (MER) have successfully demonstrated a robotic Visual Odometry capability on another world for the first time. This provides each rover with accurate knowledge of its position, allowing it to autonomously detect and compensate for any unforeseen slip encountered during a drive. It has enabled the rovers to drive safely and more effectively in highly sloped and sandy terrains and has resulted in increased mission science return by reducing the number of days required to drive into interesting areas. The MER Visual Odometry system comprises onboard software for comparing stereo pairs taken by the pointable mast-mounted 45 deg FOV Navigation cameras (NAVCAMs). The system computes an update to the 6 degree of freedom rover pose (x, y, z, roll, pitch, yaw) by tracking the motion of autonomously selected terrain features between two pairs of 256×256 stereo images. It has demonstrated good performance with high rates of successful convergence (97\% on Spirit, 95\% on Opportunity), successfully detected slip ratios as high as 125\%, and measured changes as small as 2 mm, even while driving on slopes as high as 31 deg. Visual Odometry was used over 14\% of the first 10.7 km driven by both rovers. During the first 2 years of operations, Visual Odometry evolved from an “extra credit” capability into a critical vehicle safety system. In this paper we describe our Visual Odometry algorithm, discuss several driving strategies that rely on it (including Slip Checks, Keep-out Zones, and Wheel Dragging), and summarize its results from the first 2 years of operations on Mars. © 2006 Wiley Periodicals, Inc.},
year = {2007},
}

@article{CompOmniConvVSLAM,
  title={Comparison of omnidirectional and conventional monocular systems for visual slam},
  author={Rituerto, Alejandro and Puig, Luis and Guerrero, JJ},
  journal={10th OMNIVIS with RSS},
  year={2010}
}

@INPROCEEDINGS{OmniDenseSLAM,
author={A. Pretto and E. Menegatti and E. Pagello},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={Omnidirectional dense large-scale mapping and navigation based on meaningful triangulation},
year={2011},
pages={3289-3296},
keywords={cameras;edge detection;feature extraction;image sensors;mesh generation;motion estimation;navigation;object tracking;SLAM (robots);solid modelling;topology;omnidirectional dense large-scale mapping;dense 3D maps;omnidirectional camera;egomotion estimation;robust tracking;omnidirectional image;triangle mesh subdivision;constrained Delaunay triangulation;edgelet feature extraction;structure parameter estimation;optimization framework;car roof;Three dimensional displays;Cameras;Image edge detection;Feature extraction;Tracking;Optimization;Simultaneous localization and mapping},
doi={10.1109/ICRA.2011.5980206},
ISSN={1050-4729},
month={05},}

@InProceedings{OMNIChooseLensVisual,
author="Streckel, Birger
and Koch, Reinhard",
editor="Kropatsch, Walter G.
and Sablatnig, Robert
and Hanbury, Allan",
title="Lens Model Selection for Visual Tracking",
booktitle="Pattern Recognition",
year="2005",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="41--48",
abstract="A standard approach to generate a camera pose from images of a single moving camera is Structure From Motion (SfM). When aiming on a practical implementation of SfM often a camera is needed that is lightweight and small. This work analyses which is the best camera and lens for SfM, that is small in size and available on the market. Therefore we compare cameras with fisheye and perspective lenses. It is shown that pose estimation is improved by a fisheye lens. Also some methods are discussed, how the large Field of View can be further exploited to improve the pose estimation.",
isbn="978-3-540-31942-9"
}

@ARTICLE{MobileSLAM,
author={J. Ventura and C. Arth and G. Reitmayr and D. Schmalstieg},
journal={IEEE Transactions on Visualization and Computer Graphics},
title={Global Localization from Monocular SLAM on a Mobile Phone},
year={2014},
volume={20},
number={4},
pages={531-539},
keywords={image sensors;pose estimation;SLAM (robots);smart phones;monocular SLAM;global localization method;SLAM system;mobile client;6DoF pose estimation;keyframe images;camera locations;server process;SLAM map registration;mobile device;field-of-view mobile phone camera;Simultaneous localization and mapping;Cameras;Servers;Real-time systems;Global Positioning System;Feature extraction;Mobile handsets;Image-based localization; monocular SLAM; real-time tracking; global positioning; mobile augmented reality},
doi={10.1109/TVCG.2014.27},
ISSN={1077-2626},
month={04},}

@INPROCEEDINGS{FisheyeKalibration,
author={D. Scaramuzza and A. Martinelli and R. Siegwart},
booktitle={2006 IEEE/RSJ International Conference on Intelligent Robots and Systems},
title={A Toolbox for Easily Calibrating Omnidirectional Cameras},
year={2006},
volume={},
number={},
pages={5695-5701},
keywords={image sensors;laser ranging;least squares approximations;mathematics computing;maximum likelihood estimation;series (mathematics);omnidirectional cameras;Taylor series expansion;four-step least-squares linear minimization problem;maximum likelihood criterion;3D sick laser range finder;Toolbox;Calibration;Mathematical model;Mirrors;Lenses;Layout;Smart cameras;Intelligent robots;Robot vision systems;Taylor series;Maximum likelihood estimation;catadioptric;omnidirectional;camera;calibration;toolbox},
doi={10.1109/IROS.2006.282372},
ISSN={2153-0858},
month={10},}

@book{FisheyeCorke,
  title={Robotics, Vision and Control: Fundamental Algorithms In MATLAB{\textregistered} Second, Completely Revised},
  author={Corke, Peter},
  volume={118},
  year={2017},
  publisher={Springer},
  doi = {10.1007/978-3-319-54413-7},
  chapter = {11},
  pages = {319--537}
}

@online{SceenshotsAnsel,
Title = {Capturing a 360 screenshot using Nvidia's Ansel Unreal Plugin},
author = {Loughry, Sarah},
date = {2017-06-09},
url = {https://cubebrush.co/blog/360-screenshots-in-unreal-engine},
urldate = {2018-08-20}
}

@online{MatlabFish,
title = {Fisheye Calibration Basics},
author = {{MathWorks inc.}},
date = {},
url = {https://www.mathworks.com/help/vision/ug/fisheye-calibration-basics.html},
urldate = {2019-01-22},
}

@INPROCEEDINGS{CatadioptricOmni,
author={S. K. Nayar},
booktitle={Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
title={Catadioptric omnidirectional camera},
year={1997},
volume={},
number={},
pages={482-488},
keywords={video cameras;computer vision;image processing;catadioptric omnidirectional camera;video cameras;imaging system;hemispherical field of view;omnidirectional sensor;software generation;user-selected viewing direction;magnification;spatial resolution;Cameras;Image sensors;Computer science;Spatial resolution;Navigation;Surveillance;Layout;Mirrors;Yagi-Uda antennas;Software prototyping},
doi={10.1109/CVPR.1997.609369},
ISSN={1063-6919},
month={06},}

@article{RectilinearCatadioptric,
author = {Gyeong-il Kweon and Seung Hwang-bo and Geon-hee Kim and Sun-cheol Yang and Young-hun Lee},
journal = {Appl. Opt.},
keywords = {Lens system design; Photography; Aspherics; Geometric optical design ; CCD cameras; Image sensors; Lens design; Mirror design; Modulation transfer function; Nodal points},
number = {34},
pages = {8659--8673},
publisher = {OSA},
title = {Wide-angle catadioptric lens with a rectilinear projection scheme},
volume = {45},
month = {12},
year = {2006},
url = {http://ao.osa.org/abstract.cfm?URI=ao-45-34-8659},
doi = {10.1364/AO.45.008659},
abstract = {A catadioptric wide-angle lens having a rectilinear projection scheme has been developed with a view to possible applications in the security-surveillance area. The lens has been designed for a miniature camera with a video graphics array-grade 1/3 in. color CCD sensor.The field of view of the lens is over 151{\textdegree}, and still distortion is under 1\%. Furthermore, the modulation transfer function is better than 0.3 at 70 line pairs/mm over the whole active area of the image sensor.},
}

@inproceeding{PanomorphLowCostSurvailance,
author = {Thibault, Simon and Artonne, Jean-Claude},
title = {Panomorph lenses: a low-cost solution for panoramic surveillance},
journal = {Proc.SPIE},
volume = {6203},
number = {},
year = {2006},
doi = {10.1117/12.665728},
URL = {https://doi.org/10.1117/12.665728},
eprint = {}
}

@inproceeding{PanomorphEnhancesSurvailance,
author = {Thibault, Simon},
title = {Enhanced surveillance system based on panomorph panoramic lenses},
journal = {Proc.SPIE},
volume = {6540},
number = {},
year = {2007},
doi = {10.1117/12.719495},
URL = {https://doi.org/10.1117/12.719495},
eprint = {},
}